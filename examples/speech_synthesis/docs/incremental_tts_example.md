[[Back]](..)

# Incremental Text-to-Speech

Before reading this example, it may be helpful to go through the tutorial of [offline TTS on LJSpeech](../docs/ljspeech_example.md).

In this example, we build an incremental version of [FastSpeech 2](https://arxiv.org/abs/2006.04558).

## Data preparation
See [data preparation for offline TTS](../docs/ljspeech_example.md#data-preparation)

## Training with prefix augmentation

Compared to the [training script for offline TTS](../docs/ljspeech_example.md#fastspeech2), 
the differences are as follow:
* `--task text_to_speech_augmented`
* `--user-dir examples/speech_synthesis/incremental_text_to_speech`

With the task `text_to_speech_augmented`, 
the training samples will be replaced by prefixes by `--prefix-probability`, where the default is 0.5.
The prefix lengths are defined by `--prefix-augment-ratio`. 
By default, the prefixes are 1/3 or 2/3 the length of the full sequence length.

```bash
fairseq-train ${FEATURE_MANIFEST_ROOT} --user-dir examples/speech_synthesis/incremental_text_to_speech \
  --save-dir ${SAVE_DIR} \
  --config-yaml config.yaml --train-subset train --valid-subset dev \
  --num-workers 4 --max-sentences 6 --max-update 200000 \
  --task text_to_speech_augmented --criterion fastspeech2 --arch fastspeech2 \
  --clip-norm 5.0 --n-frames-per-step 1 \
  --dropout 0.1 --attention-dropout 0.1 \
  --optimizer adam --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \
  --seed 1 --update-freq 8 --eval-inference --best-checkpoint-metric mcd_loss
```


## Inference
First, we average the last checkpoints as described in the [offline TTS example](../docs/ljspeech_example.md#inference).

### Offline inference
The trained models can perform offline TTS 
in the same way described in [offline TTS example](../docs/ljspeech_example.md#inference).

### Incremental inference
We allow a lookahead defined by `--lookahead-words`.
We keep `--batch-size` at 1 for accurate measure of computation latency for each utterance.

#### From simultaneous speech translation outputs
We synthesize speech incrementally from 
a [simultaneous speech translation](https://github.com/pytorch/fairseq/blob/main/examples/speech_to_text/docs/simulst_mustc_example.md) output file 
generated by [SimulEval](https://github.com/facebookresearch/SimulEval).
We use the input timestamps from the log files to calculate latency.

```bash
INSTANCE_LOG=path_2_instance_log/instance.log

python -m examples.speech_synthesis.incremental_text_to_speech.generate_waveform_incremental \ 
    ${FEATURE_MANIFEST_ROOT} \
    --config-yaml $CONFIG --task text_to_speech \
    --path ${CHECKPOINT_PATH} --max-tokens 50000 --spec-bwd-max-iter 32 \
    --batch-size 1 \
    --dump-waveforms --results-path $RES_DIR --lookahead-words $LOOKAHEAD \
    --vocoder hifigan \
    --incremental-tts-input-from-simuleval \
    --incremental-tts-input-text $INSTANCE_LOG
```

#### From plain text file
Alternatively, we can also directly synthesize the texts in a plain text file.
In this case, since there is no timestamp associated with input words, 
we assume all words are available when we start synthesizing the utterance.
```bash
TEXT_LOG=path_2_text_log/tts_input.txt

python -m examples.speech_synthesis.incremental_text_to_speech.generate_waveform_incremental \ 
    ${FEATURE_MANIFEST_ROOT} \
    --config-yaml $CONFIG --task text_to_speech \
    --path ${CHECKPOINT_PATH} --max-tokens 50000 --spec-bwd-max-iter 32 \
    --batch-size 1 \
    --dump-waveforms --results-path $RES_DIR --lookahead-words $LOOKAHEAD \
    --vocoder hifigan \
    --incremental-tts-input-text $TEXT_LOG
```

## Evaluation
### Quality Evaluation
For quality evaluation (CER, MCD, F0), 
we use the same metrics as the [offline TTS example](../docs/ljspeech_example.md#automatic-evaluation).

### Latency Evaluation
For each utterance, we create a latency log created in 
`$RES_DIR/outputs_incremental_lookahead${LOOKAHEAD}/${SAMPLE_ID}/wav_${SAMPLE_RATE}hz_${VOCODER}/${SAMPLE_ID}_latency.log`.

After all the utterances are synthesized, the overall latency statistics are saved in 
`$RES_DIR/outputs_incremental_lookahead${LOOKAHEAD}/wav_${SAMPLE_RATE}hz_${VOCODER}_latency.txt`

For example:
```
Avg speaking latency (s), i.e. time elapsed between last input word and end of synthesized speech: 0.96
Avg computation time (s): 0.24
Avg play time (s): 2.41
Avg discontinuity (s): 1.90
```

[[Back]](..)
